{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ST3ALT4/edgeai_robotics/blob/main/image_processing/Copy_of_Deeplearning_with_Images_Part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will implement deep convolutional neural networks for image processing tasks. If you have never implemented a neural network, this notebook will help you start with the basics and slowly move towards a stage where you can write your own networks and train them for a specific task. If you are already comfortable with the implemention of a deeplearning convolutional neural network, this will be a recap of the basics before we move to Part 2.\n",
        "Lets get started:"
      ],
      "metadata": {
        "id": "AvaWL-szikVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will begin with reading images and getting them ready for use. You have already used the 2 popular image libraries **cv2** and **PIL**. We will discuss a couple of points about them. At this point we will also talk about the deeplearning frameworks and their model formats.\n",
        "The most common frameworks are **Tensorflow , Keras , Pytorch**. Although these are widely used today, they are not the oldest ones. **Torch** and **Caffe** are among 2 frameworks used in the early days. The Matlab adaptation of caffe was called **Matcaffe** while the python adaptation was **pycaffe**. The Torch we are talking about is not the Pytorch we know today. It was a different package where the programming language was **\".lua\"**. Later it was adapted in python as **Pytorch**. Lets now start reading images and discussing formats:-"
      ],
      "metadata": {
        "id": "yfeTrbY1kaTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading with cv2\n",
        "import cv2\n",
        "\n",
        "img = cv2.imread('/content/Image_1.jpg')\n",
        "print(img.shape)\n",
        "\n",
        "# We will notice that cv2 reads the image in a numpy array with HWC format. This is the format commonly used for tensorflow and keras frameworks. Also, the image is 0-255 range."
      ],
      "metadata": {
        "id": "e47vEyUgnozp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3feb487-b928-4b49-92f2-afdac2268ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 320, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading with Pillow\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "t=transforms.ToTensor()\n",
        "im = Image.open('/content/Image_1.jpg')\n",
        "\n",
        "im = t(im)\n",
        "print(im.shape)\n",
        "\n",
        "# We will notice that PIL reads the image and returns a PIL object. This is later converted to a torch tensor with CHW format. This is the format commonly used for pytorch. Also, the image is 0-1 range."
      ],
      "metadata": {
        "id": "54eyoqYyobdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4c9078-1cb7-4463-9df0-169ac1d20d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 240, 320])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are wondering whether we always use cv2 with tensorflow and PIL with pytorch, the answer is \"not really\". We are ofcourse free to use what we want as long as we end up with the correct format. For example, while using pytorch, I may read images with cv2 but I need to convert the numpy array into a torch tensor and transpose the matrix to follow the CHW format and am good to go. Take a look at the examples below if theres any confusion."
      ],
      "metadata": {
        "id": "x4gLsJO7pd7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "img = cv2.imread('/content/Image_1.jpg')\n",
        "img = img.transpose((2,0,1))\n",
        "print(img.shape)\n",
        "img = torch.from_numpy(img)\n",
        "print(img.shape)\n",
        "img=img/255 #to bring it in 0-1 range\n",
        "#"
      ],
      "metadata": {
        "id": "CXM6ILI5qYYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc28293-ab8b-499f-a577-238bfeff9d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 240, 320)\n",
            "torch.Size([3, 240, 320])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we discussed reading single images, lets talk about reading the entire dataset using a dataloader. We will write a class for it."
      ],
      "metadata": {
        "id": "k28RldmHrIOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dataloader for an image processing (regression) task\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "import torch.utils.data as data\n",
        "from random import randrange\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "\n",
        "to_tensor = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "class DLdata:\n",
        "\n",
        "\n",
        "    def __init__(self,train=True):\n",
        "\n",
        "        data_dir = '/content/data/'\n",
        "\n",
        "\n",
        "        self.output = data_dir+'out'\n",
        "        self.input = data_dir+'inp'\n",
        "\n",
        "        self.image_names = os.listdir(self.input)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_name = self.image_names[idx]\n",
        "        out_img_name='gt_'+img_name\n",
        "        input_image = Image.open(os.path.join(self.input, img_name)).convert('L') #noisy1.jpg\n",
        "        output_image = Image.open(os.path.join(self.output, out_img_name)).convert('L') #gt_noisy1.jpg\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        x, y = input_image.size\n",
        "\n",
        "        matrix=260\n",
        "        x1 = randrange(0, x - matrix)\n",
        "        y1 = randrange(0, y - matrix)\n",
        "        input_image=input_image.crop((x1, y1, x1 + matrix, y1 + matrix))\n",
        "        output_image = output_image.crop((x1, y1, x1 + matrix, y1 + matrix))\n",
        "\n",
        "        input_image = to_tensor(input_image)\n",
        "        output_image = to_tensor(output_image)\n",
        "\n",
        "        return input_image, output_image\n",
        "\n",
        "\n",
        "\n",
        "kwargs = {'num_workers': 4,'pin_memory': True} if torch.cuda.is_available() else {}\n",
        "print('loading train')\n",
        "training_set = DLdata(train=True,)\n",
        "train_loader = DataLoader(training_set, batch_size=3, shuffle=True, **kwargs)"
      ],
      "metadata": {
        "id": "F0TCbVxyrgWo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "db25cf0e-67ef-4941-dc13-f0f57646c27f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading train\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/data/inp'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-38283f2ab37b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'num_workers'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pin_memory'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mtraining_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDLdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-38283f2ab37b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'inp'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/inp'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"nourabdoun/fruits-quality-fresh-vs-rotten\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "M2_i3i9VAgDJ",
        "outputId": "82787384-9908-4dc6-ce99-7c17bd8ac80d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/fruits-quality-fresh-vs-rotten\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataloader for a classification task\n",
        "#!unzip  /content/archive.zip\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "import torch.utils.data as data\n",
        "from random import randrange\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "#\n",
        "TRANSFORM_IMG = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(256),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n",
        "    ])\n",
        "\n",
        "train_data = torchvision.datasets.ImageFolder(root=\"/kaggle/input/fruits-quality-fresh-vs-rotten/Quality Dataset/train\", transform=TRANSFORM_IMG)\n",
        "valid_data = torchvision.datasets.ImageFolder(root=\"/kaggle/input/fruits-quality-fresh-vs-rotten/Quality Dataset/valid\", transform=TRANSFORM_IMG)\n",
        "\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(.25 * num_train))\n",
        "\n",
        "print(num_train)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_loader = data.DataLoader(train_data, batch_size=5, num_workers=4)\n",
        "\n",
        "test_loader  = data.DataLoader(valid_data, batch_size=1,num_workers=4)\n",
        "\n",
        "# train_loader = data.DataLoader(train_data, batch_size=16, sampler=train_sampler, num_workers=4)\n",
        "\n",
        "# test_loader  = data.DataLoader(valid_data, batch_size=1, sampler=valid_sampler,num_workers=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jafQmqPzbyfx",
        "outputId": "7256408c-9be0-4c06-d24a-a9da3ba34a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model for classification (from scratch)\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__() #some shit getting inheret can do some shit later like model(data) works as model.forward(data)\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Conv2d(64,128, kernel_size=3)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        self.conv4 = nn.Conv2d(128,256, kernel_size=3)\n",
        "        self.pool4 = nn.MaxPool2d(2, 2)\n",
        "        self.conv5 = nn.Conv2d(256,512, kernel_size=3)\n",
        "        self.pool5 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(18432, 256)\n",
        "        self.fc2 = nn.Linear(256, 5)  # Final output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional layers\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        print(x.shape)\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        print(x.shape)\n",
        "        x = self.pool2(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        print(x.shape)\n",
        "        x = self.pool3(x)\n",
        "        print(x.shape)\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.pool4(x)\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = self.pool5(x)\n",
        "\n",
        "        # Flattening the layer for the fully connected layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "        #print(x.shape)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        #x= F.log_softmax(x)\n",
        "\n",
        "        return x\n",
        "device = torch.device('vulkan' if torch.is_vulkan_available() else 'cuda' if torch.cuda.is_available() else'cpu')\n",
        "print(device)\n",
        "model = CNN().to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "UKJSPU2-Evq6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "618657d5-df58-4cda-8a01-e7e7bbaab6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "CNN(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=18432, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using a pretrained model for feature extraction...use when not to be trained from scratch\n",
        "\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "model = models.vgg16(pretrained=True)\n",
        "model.classifier[6] = nn.Linear(in_features=4096, out_features=5)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "962X2-dxzzUK",
        "outputId": "51171e28-9a2e-4dce-d592-b2f123b44841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=5, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):   #16,3,256,256\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model.forward(data)\n",
        "        loss = nn.CrossEntropyLoss()(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{num_train-split} ({100. * batch_idx / (num_train-split):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "def evaluate(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            pred = output.argmax(dim=1)\n",
        "            # print(pred)\n",
        "            correct += pred.eq(target).sum()\n",
        "            #print(correct.item())\n",
        "            test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
        "\n",
        "\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct.item()}/{split} ({100. * correct.item() / split:.0f}%)\\n')\n",
        "\n",
        "\n",
        "for epoch in range(5):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    evaluate(model, device, test_loader)\n",
        "torch.save(model.state_dict(), 'model_weights.pth')"
      ],
      "metadata": {
        "id": "hM2c6322Eywh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47d65a9-4376-4549-9541-9f9e2dea8e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/216 (0%)]\tLoss: 1.689620\n",
            "Train Epoch: 0 [50/216 (5%)]\tLoss: 0.000000\n",
            "Train Epoch: 0 [100/216 (9%)]\tLoss: 0.000000\n",
            "Train Epoch: 0 [150/216 (14%)]\tLoss: 1010.852051\n",
            "Train Epoch: 0 [200/216 (19%)]\tLoss: 0.000000\n",
            "Train Epoch: 0 [250/216 (23%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 604.3192, Accuracy: 24/71 (34%)\n",
            "\n",
            "Train Epoch: 1 [0/216 (0%)]\tLoss: 1409.677979\n",
            "Train Epoch: 1 [50/216 (5%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [100/216 (9%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [150/216 (14%)]\tLoss: 610.263855\n",
            "Train Epoch: 1 [200/216 (19%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [250/216 (23%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 135.6931, Accuracy: 24/71 (34%)\n",
            "\n",
            "Train Epoch: 2 [0/216 (0%)]\tLoss: 343.147522\n",
            "Train Epoch: 2 [50/216 (5%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [100/216 (9%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [150/216 (14%)]\tLoss: 234.640167\n",
            "Train Epoch: 2 [200/216 (19%)]\tLoss: 0.000000\n",
            "Train Epoch: 2 [250/216 (23%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 187.0259, Accuracy: 24/71 (34%)\n",
            "\n",
            "Train Epoch: 3 [0/216 (0%)]\tLoss: 484.934174\n",
            "Train Epoch: 3 [50/216 (5%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [100/216 (9%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [150/216 (14%)]\tLoss: 543.804199\n",
            "Train Epoch: 3 [200/216 (19%)]\tLoss: 0.000000\n",
            "Train Epoch: 3 [250/216 (23%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 239.3784, Accuracy: 24/71 (34%)\n",
            "\n",
            "Train Epoch: 4 [0/216 (0%)]\tLoss: 684.255554\n",
            "Train Epoch: 4 [50/216 (5%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [100/216 (9%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [150/216 (14%)]\tLoss: 497.834412\n",
            "Train Epoch: 4 [200/216 (19%)]\tLoss: 0.000000\n",
            "Train Epoch: 4 [250/216 (23%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 103.2980, Accuracy: 24/71 (34%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ABBRl-SLF7tV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}